{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dataset_scraping.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"BeUdfVcyFBM-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9eb25906-a2db-44a1-a8ea-03c2a88c579a"},"outputs":[{"output_type":"stream","name":"stdout","text":["['bacon', 'beef', 'chicken', 'cooked meat', 'duck', 'ham', 'kidneys', 'lamb', 'liver', 'potato', 'salami', 'sausages', 'pork', 'veal', 'apple', 'apricot', 'banana', 'blackberry', 'blackcurrant', 'blueberry', 'cherry', 'coconut', 'fig', 'gooseberry', 'grape', 'grapefruit', 'kiwi', 'lemon', 'lime', 'mango', 'melon', 'orange', 'pizza', 'burger', 'burrito', 'kebab', 'pasta', 'salad', 'peach', 'pear', 'pineapple', 'plum', 'pomegranate', 'raspberry', 'redcurrant', 'rhubarb', 'strawberry', 'anchovy', 'cod', 'haddock', 'herring', 'kipper', 'mackerel', 'pilchard', 'plaice', 'salmon', 'sardine', 'sole', 'trout', 'tuna', 'biscuits', 'chocolate', 'crisps', 'hummus', 'nuts', 'olives', 'peanuts', 'sweets', 'walnuts', 'ketchup', 'mayonnaise', 'mustard', 'pepper', 'salt', 'vinaigrette', 'vinegar', 'cereal', 'cornflakes', 'honey', 'jam', 'marmalade', 'muesli', 'porridge', 'toast', 'kangaroo', 'eggs', 'lobster', 'fish', 'turkey', 'fast food', 'onion fries', 'french fries', 'fried chicken', 'taco', 'noodles', 'muffin', 'hot dog', 'nuggets', 'sausage', 'ice cream']\n"]}],"source":["import requests\n","from bs4 import BeautifulSoup\n","import csv\n","import os\n","import shutil\n","import numpy as np\n","import cv2\n","from google.colab.patches import cv2_imshow\n","import urllib.request as ur #used for request images for avoiding problems with cv2 read\n","\n","# Separate images into folders\n","separate =   False #@param {type: \"boolean\"}\n","# Base path where images and csv file are going to be stored\n","BASE_PATH = '/content' #@param {type: \"string\"}\n","\n","def wikipedia_get_images(title, writer):\n","  session = requests.Session()\n","\n","  # Using wikipedia api for obtaining a json from an article\n","  URL = \"Ut\"\n","  \n","  PARAMS = {\n","    \"action\": \"parse\",\n","    \"format\": \"json\",\n","    \"page\": title\n","  }\n","\n","  request = session.get(url=URL, params=PARAMS)\n","\n","  request_json = request.json()\n","  if \"parse\" in request_json:\n","    # Take the html content of the json article\n","    page = (request_json[\"parse\"][\"text\"][\"*\"])\n","\n","    soup = BeautifulSoup(page, 'html.parser')\n","    \n","    # Get only the divs where there are images\n","    thumb_divs = soup.findAll(\"div\", {\"class\": \"thumbinner\"})\n","\n","    images = []\n","    id=0\n","\n","    if separate:\n","      # Create the directory where the images of the article are saved\n","      os.mkdir(BASE_PATH+'/images/'+title)\n","    \n","    # Iterate all the divs with images\n","    for div in thumb_divs:\n","      # Get the image url\n","      if div.findAll(\"img\"):\n","        image_url = div.findAll(\"img\")[0]['src']\n","        # Send a get request to the url\n","        image_response = requests.get('http:'+image_url, stream=True).raw\n","        # Get the caption of the image\n","        caption = div.findAll(\"div\")[0].text\n","        # Generate the image id\n","        imageid = title+str(id)\n","        # If the image div has caption we save the image and its caption\n","        if caption:\n","\n","          # Save the image file\n","          image_np = np.asarray(bytearray(image_response.read()), dtype=\"uint8\")\n","          image_cv2 = cv2.imdecode(image_np, cv2.IMREAD_UNCHANGED)\n","          if image_cv2 is not None:\n","            img_stretch = cv2.resize(image_cv2, (64, 64))\n","            if separate:\n","              cv2.imwrite(BASE_PATH+'/images/'+title+'/'+imageid+'.png', img_stretch)\n","            else:\n","              cv2.imwrite(BASE_PATH+'/images/'+imageid+'.png', img_stretch)\n","            # Write the row with the imageid-caption pair\n","            row = [imageid, caption]\n","            writer.writerow(row)\n","            id+=1\n","\n","\n","# Array of the articles where images are taken\n","words_list = []\n","import csv\n","# Place a file articles.csv with the number of articles from which\n","# images will be downloaded (one article name on each line) \n","if os.path.exists(BASE_PATH+'/articles.csv'):\n","  with open('articles.csv', newline='') as csvfile:\n","    for row in csvfile:\n","      words_list.append(row.strip())\n","else:\n","  words_list = ['cat', 'dog', 'bird', 'cattle', 'monkey']\n","print(words_list)\n","\n","if os.path.exists(BASE_PATH+'/images'):\n","  shutil.rmtree(BASE_PATH+'/images')\n","os.mkdir(BASE_PATH+'/images')\n","\n","# Generate csv for imageid-caption pairs\n","file = open(BASE_PATH+'/captions.csv', 'w')\n","header = ['image_id', 'caption']\n","writer = csv.writer(file)\n","writer.writerow(header)\n","# Iterate the array of articles' name\n","for title in words_list:\n","  # Download every image of the article\n","  wikipedia_get_images(title, writer)\n","  \n","file.close()\n","\n"]},{"cell_type":"code","source":["# Copy to Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","!cp -r /content/images /content/drive/MyDrive/\"Colab Notebooks\"/TFG/dataset\n","!cp /content/captions.csv /content/drive/MyDrive/\"Colab Notebooks\"/TFG/dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g0NqoGQizeJy","outputId":"2375d69b-1486-47d5-af6c-ee6e7d8b67e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]}]}